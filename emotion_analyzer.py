import torch
from transformers import BertForSequenceClassification, AutoTokenizer
import numpy as np
import re

# Í∞êÏ†ï Î†àÏù¥Î∏î
EMOTION_LABELS = ['Î∂ÑÎÖ∏', 'Ïä¨Ìîî', 'Î∂àÏïà', 'ÏÉÅÏ≤ò', 'ÎãπÌô©', 'Í∏∞ÏÅ®']

def split_sentences(text):
    """
    ÌÖçÏä§Ìä∏Î•º Î¨∏Ïû• Îã®ÏúÑÎ°ú Î∂ÑÎ¶¨ (Í∞úÏÑ†Îêú Î≤ÑÏ†Ñ)
    """
    # ÌïúÍµ≠Ïñ¥ Î¨∏Ïû• Íµ¨Î∂ÑÏûê
    text = text.strip()
    
    # Î¨∏Ïû• Ï¢ÖÍ≤∞ Î∂ÄÌò∏Î°ú Î∂ÑÎ¶¨
    sentences = re.split(r'([.!?]+[\s]*)', text)
    
    # Î∂ÑÎ¶¨Îêú Íµ¨Î∂ÑÏûêÏôÄ Î¨∏Ïû• Ïû¨Í≤∞Ìï©
    result = []
    for i in range(0, len(sentences) - 1, 2):
        sentence = (sentences[i] + sentences[i + 1]).strip()
        if sentence:
            result.append(sentence)
    
    # ÎßàÏßÄÎßâ Î¨∏Ïû• (Íµ¨Î∂ÑÏûê ÏóÜÏùÑ Ïàò ÏûàÏùå)
    if len(sentences) % 2 == 1 and sentences[-1].strip():
        result.append(sentences[-1].strip())
    
    # Î¨∏Ïû•Ïù¥ ÏóÜÏúºÎ©¥ ÏõêÎ≥∏ ÌÖçÏä§Ìä∏ Î∞òÌôò
    if not result:
        result = [text]
    
    return result


def analyze_emotion_with_model(text, model_path='emotion_model_best.pth'):
    """
    Í∞úÏÑ†Îêú Í∞êÏ†ï Î∂ÑÏÑù (Î¨∏Ïû• Îã®ÏúÑ Î∂ÑÏÑù + Í∞ÄÏ§ë ÌèâÍ∑†)
    
    Parameters:
    - text: Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏
    - model_path: Î™®Îç∏ ÌååÏùº Í≤ΩÎ°ú
    
    Returns:
    - dict: {'Î∂ÑÎÖ∏': 10.5, 'Ïä¨Ìîî': 20.3, ...}
    """
    print(f"\n{'='*60}")
    print("Í∞êÏ†ï Î∂ÑÏÑù ÏãúÏûë")
    print(f"{'='*60}")
    
    # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ÎîîÎ∞îÏù¥Ïä§: {device}")
    
    # Î™®Îç∏ Î°úÎìú
    model = BertForSequenceClassification.from_pretrained(
        'monologg/kobert',
        num_labels=6,
        trust_remote_code=True
    )
    
    # ÌïôÏäµÎêú Í∞ÄÏ§ëÏπò Î°úÎìú
    try:
        checkpoint = torch.load(model_path, map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"‚úì Î™®Îç∏ Î°úÎìú ÏôÑÎ£å: {model_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    model.to(device)
    model.eval()
    
    # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
    tokenizer = AutoTokenizer.from_pretrained('monologg/kobert', trust_remote_code=True)
    
    # ÌÖçÏä§Ìä∏Î•º Î¨∏Ïû•ÏúºÎ°ú Î∂ÑÎ¶¨
    sentences = split_sentences(text)
    print(f"\nüìù Î∂ÑÏÑùÌï† Î¨∏Ïû• Í∞úÏàò: {len(sentences)}")
    
    # Î¨∏Ïû•Î≥Ñ Í∞êÏ†ï Î∂ÑÏÑù
    sentence_emotions = []
    
    for i, sentence in enumerate(sentences, 1):
        if not sentence.strip():
            continue
        
        print(f"\n[Î¨∏Ïû• {i}] {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
        
        # ÌÜ†ÌÅ∞Ìôî
        inputs = tokenizer(
            sentence,
            return_tensors='pt',
            max_length=128,
            padding='max_length',
            truncation=True
        )
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        # ÏòàÏ∏°
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            
            # SoftmaxÎ°ú ÌôïÎ•† Î≥ÄÌôò
            probs = torch.softmax(logits, dim=1)[0]
            probs = probs.cpu().numpy()
        
        # Î¨∏Ïû•Î≥Ñ Í∞êÏ†ï Ï†ÄÏû•
        sentence_emotion = {
            label: float(prob * 100) 
            for label, prob in zip(EMOTION_LABELS, probs)
        }
        
        sentence_emotions.append(sentence_emotion)
        
        # Î¨∏Ïû•Î≥Ñ Í≤∞Í≥º Ï∂úÎ†•
        sorted_emotions = sorted(
            sentence_emotion.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        print(f"   Ï£ºÏöî Í∞êÏ†ï: {sorted_emotions[0][0]} ({sorted_emotions[0][1]:.1f}%)")
    
    # Ï†ÑÏ≤¥ Í∞êÏ†ï ÌÜµÌï© (Í∞ÄÏ§ë ÌèâÍ∑†)
    if len(sentence_emotions) == 1:
        # Î¨∏Ïû•Ïù¥ 1Í∞úÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
        final_emotions = sentence_emotions[0]
    else:
        # Ïó¨Îü¨ Î¨∏Ïû•Ïùò Í∞êÏ†ïÏùÑ ÌèâÍ∑†
        final_emotions = {label: 0.0 for label in EMOTION_LABELS}
        
        for sent_emotion in sentence_emotions:
            for label in EMOTION_LABELS:
                final_emotions[label] += sent_emotion[label]
        
        # ÌèâÍ∑† Í≥ÑÏÇ∞
        num_sentences = len(sentence_emotions)
        for label in EMOTION_LABELS:
            final_emotions[label] /= num_sentences
    
    # Ï†ïÍ∑úÌôî (Ìï©Ïù¥ 100%Í∞Ä ÎêòÎèÑÎ°ù)
    total = sum(final_emotions.values())
    if total > 0:
        final_emotions = {
            label: (score / total) * 100 
            for label, score in final_emotions.items()
        }
    
    # ÏµúÏ¢Ö Í≤∞Í≥º Ï∂úÎ†•
    print(f"\n{'='*60}")
    print("ÏµúÏ¢Ö Í∞êÏ†ï Î∂ÑÏÑù Í≤∞Í≥º")
    print(f"{'='*60}")
    
    sorted_final = sorted(
        final_emotions.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    for emotion, score in sorted_final:
        bar = "‚ñà" * int(score / 5)
        print(f"{emotion:6s}: {score:5.1f}% {bar}")
    
    return final_emotions


def analyze_emotion_advanced(text, model_path='emotion_model_best.pth', 
                             method='sentence_avg'):
    """
    Í≥†Í∏â Í∞êÏ†ï Î∂ÑÏÑù (Ïó¨Îü¨ Î∞©Î≤ï ÏÑ†ÌÉù Í∞ÄÎä•)
    
    Parameters:
    - text: Î∂ÑÏÑùÌï† ÌÖçÏä§Ìä∏
    - model_path: Î™®Îç∏ ÌååÏùº Í≤ΩÎ°ú
    - method: 'sentence_avg', 'weighted', 'max_pool', 'whole'
    
    Returns:
    - dict: {'Î∂ÑÎÖ∏': 10.5, 'Ïä¨Ìîî': 20.3, ...}
    """
    print(f"\nüîç Î∂ÑÏÑù Î∞©Î≤ï: {method}")
    
    # ÎîîÎ∞îÏù¥Ïä§ ÏÑ§Ï†ï
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Î™®Îç∏ Î°úÎìú
    model = BertForSequenceClassification.from_pretrained(
        'monologg/kobert',
        num_labels=6
    )
    
    try:
        checkpoint = torch.load(model_path, map_location=device)
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
    except Exception as e:
        print(f"‚ö†Ô∏è  Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    model.to(device)
    model.eval()
    
    tokenizer = AutoTokenizer.from_pretrained('monologg/kobert')
    
    # Î¨∏Ïû• Î∂ÑÎ¶¨
    sentences = split_sentences(text)
    print(f"üìù Î¨∏Ïû• Í∞úÏàò: {len(sentences)}")
    
    # Î∞©Î≤ï 1: Ï†ÑÏ≤¥ ÌÖçÏä§Ìä∏ Ìïú Î≤àÏóê Î∂ÑÏÑù (Í∏∞Ï°¥ Î∞©Ïãù)
    if method == 'whole':
        inputs = tokenizer(
            text,
            return_tensors='pt',
            max_length=128,
            padding='max_length',
            truncation=True
        )
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)[0]
            probs = probs.cpu().numpy()
        
        return {label: float(prob * 100) for label, prob in zip(EMOTION_LABELS, probs)}
    
    # Î∞©Î≤ï 2~4: Î¨∏Ïû•Î≥Ñ Î∂ÑÏÑù
    sentence_emotions = []
    sentence_lengths = []
    
    for sentence in sentences:
        if not sentence.strip():
            continue
        
        inputs = tokenizer(
            sentence,
            return_tensors='pt',
            max_length=128,
            padding='max_length',
            truncation=True
        )
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)[0]
            probs = probs.cpu().numpy()
        
        sentence_emotion = {
            label: float(prob * 100) 
            for label, prob in zip(EMOTION_LABELS, probs)
        }
        
        sentence_emotions.append(sentence_emotion)
        sentence_lengths.append(len(sentence))
    
    if not sentence_emotions:
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    # Î∞©Î≤ï 2: Îã®Ïàú ÌèâÍ∑†
    if method == 'sentence_avg':
        final_emotions = {label: 0.0 for label in EMOTION_LABELS}
        
        for sent_emotion in sentence_emotions:
            for label in EMOTION_LABELS:
                final_emotions[label] += sent_emotion[label]
        
        for label in EMOTION_LABELS:
            final_emotions[label] /= len(sentence_emotions)
    
    # Î∞©Î≤ï 3: Î¨∏Ïû• Í∏∏Ïù¥ Í∞ÄÏ§ë ÌèâÍ∑† (Í∏¥ Î¨∏Ïû•Ïù¥ Îçî Ï§ëÏöî)
    elif method == 'weighted':
        final_emotions = {label: 0.0 for label in EMOTION_LABELS}
        total_length = sum(sentence_lengths)
        
        for sent_emotion, length in zip(sentence_emotions, sentence_lengths):
            weight = length / total_length
            for label in EMOTION_LABELS:
                final_emotions[label] += sent_emotion[label] * weight
    
    # Î∞©Î≤ï 4: Max Pooling (Í∞Å Í∞êÏ†ïÏùò ÏµúÎåìÍ∞í)
    elif method == 'max_pool':
        final_emotions = {label: 0.0 for label in EMOTION_LABELS}
        
        for label in EMOTION_LABELS:
            max_score = max(sent_emotion[label] for sent_emotion in sentence_emotions)
            final_emotions[label] = max_score
    
    else:
        raise ValueError(f"Unknown method: {method}")
    
    # Ï†ïÍ∑úÌôî
    total = sum(final_emotions.values())
    if total > 0:
        final_emotions = {
            label: (score / total) * 100 
            for label, score in final_emotions.items()
        }
    
    # Í≤∞Í≥º Ï∂úÎ†•
    print(f"\nÏµúÏ¢Ö Í∞êÏ†ï Î∂ÑÏÑù Í≤∞Í≥º ({method}):")
    for emotion, score in sorted(final_emotions.items(), key=lambda x: x[1], reverse=True):
        print(f"  {emotion}: {score:.1f}%")
    
    return final_emotions

