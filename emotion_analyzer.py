import torch
from transformers import BertForSequenceClassification, AutoTokenizer
import numpy as np
import re

# ê°ì • ë ˆì´ë¸”
EMOTION_LABELS = ['ë¶„ë…¸', 'ìŠ¬í””', 'ë¶ˆì•ˆ', 'ìƒì²˜', 'ë‹¹í™©', 'ê¸°ì¨']

def split_sentences(text):
    """
    í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ê°œì„ ëœ í•œêµ­ì–´ ë¶„ë¦¬)
    """
    text = text.strip()
    
    # ë¬¸ì¥ ì¢…ê²° ë¶€í˜¸ë¡œ ë¶„ë¦¬
    sentences = re.split(r'([.!?]+[\s]*)', text)
    
    # ë¶„ë¦¬ëœ êµ¬ë¶„ìì™€ ë¬¸ì¥ ì¬ê²°í•©
    result = []
    for i in range(0, len(sentences) - 1, 2):
        sentence = (sentences[i] + sentences[i + 1]).strip()
        if sentence and len(sentence) > 2:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            result.append(sentence)
    
    # ë§ˆì§€ë§‰ ë¬¸ì¥ (êµ¬ë¶„ì ì—†ì„ ìˆ˜ ìˆìŒ)
    if len(sentences) % 2 == 1 and sentences[-1].strip():
        if len(sentences[-1].strip()) > 2:
            result.append(sentences[-1].strip())
    
    # ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
    if not result:
        result = [text]
    
    return result


def analyze_emotion_with_model(text, model_path='emotion_model_best.pth'):
    """
    ê°ì • ë¶„ì„ (Weighted ë°©ì‹ - ë¬¸ì¥ ê¸¸ì´ ê°€ì¤‘ í‰ê· )
    
    Parameters:
    - text: ë¶„ì„í•  í…ìŠ¤íŠ¸
    - model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    
    Returns:
    - dict: {'ë¶„ë…¸': 10.5, 'ìŠ¬í””': 20.3, ...}
    """
    print(f"\n{'='*60}")
    print("ğŸ” ê°ì • ë¶„ì„ ì‹œì‘ (Weighted ë°©ì‹)")
    print(f"{'='*60}")
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ë””ë°”ì´ìŠ¤: {device}")
    
    # KoBERT ëª¨ë¸ ë¡œë“œ
    try:
        print("ğŸ“¦ KoBERT ëª¨ë¸ ë¡œë“œ ì¤‘...")
        model = BertForSequenceClassification.from_pretrained(
            'monologg/kobert',
            num_labels=6,
            trust_remote_code=True  # âœ… í•„ìˆ˜!
        )
        print("âœ“ ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        try:
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            print(f"âœ“ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ: {model_path}")
        except FileNotFoundError:
            print(f"âš ï¸  í•™ìŠµëœ ê°€ì¤‘ì¹˜ íŒŒì¼ ì—†ìŒ: {model_path}")
            print("âš ï¸  ê¸°ë³¸ KoBERT ëª¨ë¸ë¡œ ë¶„ì„í•©ë‹ˆë‹¤ (ì •í™•ë„ ë‚®ì„ ìˆ˜ ìˆìŒ)")
        except Exception as e:
            print(f"âš ï¸  ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("âš ï¸  ê¸°ë³¸ KoBERT ëª¨ë¸ë¡œ ë¶„ì„í•©ë‹ˆë‹¤")
        
        model.to(device)
        model.eval()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        # ì˜¤ë¥˜ ì‹œ ê· ë“± ë¶„í¬ ë°˜í™˜
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    try:
        print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(
            'monologg/kobert',
            trust_remote_code=True  # âœ… í•„ìˆ˜!
        )
        print("âœ“ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„ë¦¬
    sentences = split_sentences(text)
    print(f"\nğŸ“ ë¶„ì„í•  ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}")
    
    # ë¬¸ì¥ë³„ ê°ì • ë¶„ì„
    sentence_emotions = []
    sentence_lengths = []
    
    for i, sentence in enumerate(sentences, 1):
        if not sentence.strip():
            continue
        
        print(f"\n[ë¬¸ì¥ {i}/{len(sentences)}] {sentence[:50]}{'...' if len(sentence) > 50 else ''}")
        
        # í† í°í™”
        try:
            inputs = tokenizer(
                sentence,
                return_tensors='pt',
                max_length=128,
                padding='max_length',
                truncation=True
            )
            inputs = {key: val.to(device) for key, val in inputs.items()}
            
            # ì˜ˆì¸¡
            with torch.no_grad():
                outputs = model(**inputs)
                logits = outputs.logits
                probs = torch.softmax(logits, dim=1)[0]
                probs = probs.cpu().numpy()
            
            # ë¬¸ì¥ë³„ ê°ì • ì €ì¥
            sentence_emotion = {
                label: float(prob * 100) 
                for label, prob in zip(EMOTION_LABELS, probs)
            }
            
            sentence_emotions.append(sentence_emotion)
            sentence_lengths.append(len(sentence))
            
            # ë¬¸ì¥ë³„ ê²°ê³¼ ì¶œë ¥
            sorted_emotions = sorted(
                sentence_emotion.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            print(f"   ì£¼ìš” ê°ì •: {sorted_emotions[0][0]} ({sorted_emotions[0][1]:.1f}%)")
            print(f"   ë¬¸ì¥ ê¸¸ì´: {len(sentence)}ì (ê°€ì¤‘ì¹˜: {len(sentence)/sum([len(s) for s in sentences]):.2%})")
            
        except Exception as e:
            print(f"   âš ï¸ ë¬¸ì¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            continue
    
    # ë¶„ì„ ì‹¤íŒ¨ ì‹œ
    if not sentence_emotions:
        print("\nâŒ ëª¨ë“  ë¬¸ì¥ ë¶„ì„ ì‹¤íŒ¨")
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    # âœ… Weighted ë°©ì‹: ë¬¸ì¥ ê¸¸ì´ ê¸°ë°˜ ê°€ì¤‘ í‰ê· 
    print(f"\n{'='*60}")
    print("âš–ï¸  ê°€ì¤‘ í‰ê·  ê³„ì‚° ì¤‘...")
    print(f"{'='*60}")
    
    final_emotions = {label: 0.0 for label in EMOTION_LABELS}
    total_length = sum(sentence_lengths)
    
    for i, (sent_emotion, length) in enumerate(zip(sentence_emotions, sentence_lengths), 1):
        weight = length / total_length
        print(f"ë¬¸ì¥ {i}: ê¸¸ì´ {length}ì â†’ ê°€ì¤‘ì¹˜ {weight:.2%}")
        
        for label in EMOTION_LABELS:
            final_emotions[label] += sent_emotion[label] * weight
    
    # ì •ê·œí™” (í•©ì´ 100%ê°€ ë˜ë„ë¡)
    total = sum(final_emotions.values())
    if total > 0:
        final_emotions = {
            label: (score / total) * 100 
            for label, score in final_emotions.items()
        }
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print("âœ¨ ìµœì¢… ê°ì • ë¶„ì„ ê²°ê³¼ (Weighted)")
    print(f"{'='*60}")
    
    sorted_final = sorted(
        final_emotions.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    for i, (emotion, score) in enumerate(sorted_final, 1):
        bar = "â–ˆ" * int(score / 3)
        print(f"{i}. {emotion:6s}: {score:5.1f}% {bar}")
    
    print(f"{'='*60}\n")
    
    return final_emotions


def analyze_emotion_simple(text, model_path='emotion_model_best.pth'):
    """
    ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (ì „ì²´ í…ìŠ¤íŠ¸ í•œ ë²ˆì—)
    Weighted ë°©ì‹ë³´ë‹¤ ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ëŠ” ë‚®ìŒ
    
    Parameters:
    - text: ë¶„ì„í•  í…ìŠ¤íŠ¸
    - model_path: í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
    
    Returns:
    - dict: {'ë¶„ë…¸': 10.5, 'ìŠ¬í””': 20.3, ...}
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # ëª¨ë¸ ë¡œë“œ
    try:
        model = BertForSequenceClassification.from_pretrained(
            'monologg/kobert',
            num_labels=6,
            trust_remote_code=True
        )
        
        # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        try:
            checkpoint = torch.load(model_path, map_location=device, weights_only=False)
            model.load_state_dict(checkpoint.get('model_state_dict', checkpoint))
        except:
            pass
        
        model.to(device)
        model.eval()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    # í† í¬ë‚˜ì´ì €
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            'monologg/kobert',
            trust_remote_code=True
        )
    except Exception as e:
        print(f"âŒ í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {label: 100.0/6 for label in EMOTION_LABELS}
    
    # í† í°í™” ë° ì˜ˆì¸¡
    try:
        inputs = tokenizer(
            text,
            return_tensors='pt',
            max_length=128,
            padding='max_length',
            truncation=True
        )
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)[0]
            probs = probs.cpu().numpy()
        
        emotions = {
            label: float(prob * 100) 
            for label, prob in zip(EMOTION_LABELS, probs)
        }
        
        return emotions
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {label: 100.0/6 for label in EMOTION_LABELS}